{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anany\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anany\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "import xml.etree.ElementTree as ET\n",
    "import tensorflow as tf\n",
    "from argparse import Namespace\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is to remove stopwords to create relation in KG\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse XML\n",
    "tree = ET.parse('medicine_kg.xml')\n",
    "root = tree.getroot()\n",
    "medicine_kg = []\n",
    "synonym_ref = {}\n",
    "for i in range(len(root)):\n",
    "    drug_name = root[i].find(\"{http://www.drugbank.ca}name\").text\n",
    "    \n",
    "    synonym_ref[drug_name] = []\n",
    "    # This is needed for comparing the names in prescription.csv with drugbank id to form patient medicine bipartite graph\n",
    "    synonym_element = root[i].find(\"{http://www.drugbank.ca}synonyms\")\n",
    "    for synonym in synonym_element:\n",
    "        if synonym.text is not None: \n",
    "            synonym_ref[drug_name].append(synonym.text.lower())\n",
    "    if len(synonym_ref[drug_name]) == 0:\n",
    "        synonym_ref[drug_name].append(drug_name.lower())\n",
    "    \n",
    "    # This is needed for finding drug to drug interaction and constructing the medicine knowledge graph \n",
    "    interaction_element = root[i].find(\"{http://www.drugbank.ca}drug-interactions\")\n",
    "    \n",
    "    \n",
    "    for interaction in interaction_element:\n",
    "        interaction_name = interaction[1].text\n",
    "        description = interaction[2].text\n",
    "        replace_arr = [\"may\", \"combined\", \".\",\"risk\", \"severity\", \",\", 'used', 'combination'] + drug_name.lower().split(\" \") + interaction_name.lower().split(\" \")\n",
    "        \n",
    "        description_arr = remove_stopwords(description)\n",
    "        cleaned_description = [w for w in description_arr if not w.lower() in replace_arr]\n",
    "        find_adj = [\"increases\", \"increased\", \"increase\", \"decreases\", \"decreased\", \"decrease\"]\n",
    "        found_adj = [x for x in cleaned_description if x in find_adj]\n",
    "        \n",
    "        if len(found_adj) > 0:\n",
    "            cleaned_description.remove(found_adj[0])\n",
    "            if found_adj[0].find('increase') != -1:\n",
    "                cleaned_description.insert(0, 'increases')\n",
    "            else:\n",
    "                cleaned_description.insert(0, 'decreases')\n",
    "        relation = {\n",
    "            'source': drug_name,\n",
    "            'edge': ' '.join(cleaned_description),\n",
    "            'target': interaction_name\n",
    "            \n",
    "        }\n",
    "        medicine_kg.append(relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting medicine knowledge graph triplets to train, test and valid to calculate embedding by TransD\n",
    "medicine_kg_df = pd.DataFrame.from_records(medicine_kg)\n",
    "X_train, X_test_val = train_test_split(medicine_kg_df, test_size=0.6)\n",
    "X_test, X_val = train_test_split(X_test_val, test_size=0.5)\n",
    "X_train.to_csv('./med-train.csv',sep='\\t')\n",
    "X_val.to_csv('./med-valid.csv',sep='\\t')\n",
    "X_test.to_csv('./med-test.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIMIC iii prescription has patient to drug\n",
    "prescription_df = pd.read_csv('PRESCRIPTIONS.csv')\n",
    "prescription_df = prescription_df[['subject_id', 'drug', 'dose_val_rx', 'dose_unit_rx', 'form_unit_disp']]\n",
    "prescription_df = prescription_df[(prescription_df['form_unit_disp'] != 'SYR') & (prescription_df['form_unit_disp'] != 'BAG') & (prescription_df['form_unit_disp'] != 'CAN')]\n",
    "prescription_df = prescription_df[prescription_df['drug'].notna()]\n",
    "prescription_df['drug'] = prescription_df['drug'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the drugs are not in standard form, look in synonym from drugbank\n",
    "drugs = prescription_df['drug'].values\n",
    "drugs = list(set(drugs))\n",
    "final_drugs = {}\n",
    "synonyms = list(synonym_ref.values())\n",
    "actual_drug = list(synonym_ref.keys())\n",
    "for i in drugs:\n",
    "    for j in range(0, len(synonyms)):\n",
    "        if i in synonyms[j]:\n",
    "            final_drugs[i] = actual_drug[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = list(final_drugs.keys())\n",
    "col2 = list(final_drugs.values())\n",
    "data = {'drug': col1, 'actual_drug': col2}\n",
    "drug_df = pd.DataFrame.from_dict(data)\n",
    "drug_df = drug_df[drug_df['drug'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the drug name with the drugname in drugbank to connect to medicine knowledge graph\n",
    "final_df = pd.merge(prescription_df, drug_df, how='left', left_on='drug', right_on='drug')\n",
    "final_df = final_df[final_df['actual_drug'].notna()]\n",
    "final_df.drop('drug', 1, inplace=True)\n",
    "final_df.rename(columns={'actual_drug': 'drug'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create count column with number of times the drug prescribed to the patient\n",
    "pm_df = final_df.groupby(['subject_id', 'drug']).size().to_frame('count').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct bipartite graph\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(pm_df['subject_id'], bipartite=0)\n",
    "G.add_nodes_from(pm_df['drug'], bipartite=1)\n",
    "G.add_weighted_edges_from(\n",
    "    [(row['subject_id'], row['drug'], row['count']) for idx, row in pm_df.iterrows()], weight='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self,G):\n",
    "        self.g=G\n",
    "        self.num_of_nodes=self.g.number_of_nodes()\n",
    "        self.num_of_edges=self.g.number_of_edges()\n",
    "        self.edges_raw=list(self.g.edges(data=True))\n",
    "        self.nodes_raw=list(self.g.nodes())\n",
    "        self.edge_distribution=np.array([attr['weight'] for _,_,attr in self.edges_raw],dtype=np.float32)\n",
    "        self.edge_distribution/=np.sum(self.edge_distribution)\n",
    "        self.node_negative_distribution=np.power(np.array([self.g.degree(node) for node in self.nodes_raw],dtype=np.float32),0.75)\n",
    "        self.node_negative_distribution/=np.sum(self.node_negative_distribution)\n",
    "        self.node2idx={}\n",
    "        self.idx2node={}\n",
    "        for idx,node in enumerate(self.nodes_raw):\n",
    "            self.node2idx[node]=idx\n",
    "            self.idx2node[idx]=node\n",
    "        self.edges=[(self.node2idx[u],self.node2idx[v]) for u,v,_ in self.edges_raw]\n",
    "        \n",
    "    def fetch_batch(self,batch_size=5,K=3):\n",
    "        edge_batch_idx=np.random.choice(self.num_of_edges,size=batch_size,p=self.edge_distribution)\n",
    "        u_i=[]\n",
    "        u_j=[]\n",
    "        label=[]\n",
    "        for edge_idx in edge_batch_idx:\n",
    "            edge=self.edges[edge_idx]\n",
    "            if np.random.rand()>0.5:\n",
    "                edge=(edge[1],edge[0])\n",
    "            u_i.append(edge[0])\n",
    "            u_j.append(edge[1])\n",
    "            label.append(1)\n",
    "            for i in range(K):\n",
    "                while True:\n",
    "                    negative_node=np.random.choice(self.num_of_nodes,p=self.node_negative_distribution)\n",
    "                    if not self.g.has_edge(self.idx2node[edge[0]],self.idx2node[negative_node]):\n",
    "                        break\n",
    "                u_i.append(edge[0])\n",
    "                u_j.append(negative_node)\n",
    "                label.append(-1)\n",
    "        return u_i,u_j,label\n",
    "        \n",
    "    def embedding_mapping(self,embedding):\n",
    "        return {node:embedding[self.node2idx[node]] for node in self.nodes_raw}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LINE:\n",
    "    def __init__(self,args):\n",
    "        tf.compat.v1.disable_eager_execution()\n",
    "        self.u_i=tf.compat.v1.placeholder(dtype=tf.int32,shape=[args.batch_size*(args.K+1)],name='u_i')\n",
    "        self.u_j=tf.compat.v1.placeholder(dtype=tf.int32,shape=[args.batch_size*(args.K+1)],name='u_j')\n",
    "        self.label=tf.compat.v1.placeholder(dtype=tf.float32,shape=[args.batch_size*(args.K+1)],name='label')\n",
    "        with tf.compat.v1.variable_scope('embedding',reuse=True):\n",
    "            self.embedding=tf.Variable(tf.compat.v1.truncated_normal([args.num_of_nodes,args.embedding_dim]))\n",
    "        self.u_i_embedding=tf.nn.embedding_lookup(self.embedding,self.u_i)\n",
    "        if args.proximity=='first-order':\n",
    "            self.u_j_embedding=tf.nn.embedding_lookup(self.embedding,self.u_j)\n",
    "        elif args.proximity=='second-order':\n",
    "            with tf.compat.v1.variable_scope('context_embedding',reuse=True):\n",
    "                self.context_embedding=tf.Variable(tf.compat.v1.truncated_normal([args.num_of_nodes,args.embedding_dim]))\n",
    "            self.u_j_embedding=tf.nn.embedding_lookup(self.context_embedding,self.u_j)\n",
    "        self.inner_product=tf.reduce_sum(self.u_i_embedding*self.u_j_embedding,axis=1)\n",
    "        self.loss=-tf.reduce_mean(tf.compat.v1.log_sigmoid(self.label*self.inner_product))\n",
    "        self.learning_rate=tf.compat.v1.placeholder(dtype=tf.float32,name='learning_rate')\n",
    "        self.optimizer=tf.compat.v1.train.RMSPropOptimizer(learning_rate=self.learning_rate)\n",
    "        self.train_op=self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating LINE embedding for the bipartite graph\n",
    "def train(graph):\n",
    "    args=Namespace()\n",
    "    args.proximity='second-order'\n",
    "    args.embedding_dim=50\n",
    "    args.num_batches=100\n",
    "    args.K=3\n",
    "    args.batch_size=5\n",
    "    args.learning_rate=0.001\n",
    "    \n",
    "    data_loader=DataLoader(graph)\n",
    "    args.num_of_nodes=data_loader.num_of_nodes\n",
    "    model=LINE(args)\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        tf.compat.v1.global_variables_initializer().run()\n",
    "        initial_embedding=sess.run(model.embedding)\n",
    "        learning_rate=args.learning_rate\n",
    "        for i in range(args.num_batches):\n",
    "            u_i,u_j,label=data_loader.fetch_batch(batch_size=args.batch_size,K=args.K)\n",
    "            feed_dict={model.u_i:u_i,model.u_j:u_j,model.label:label,model.learning_rate:learning_rate}\n",
    "            loss,_=sess.run([model.loss,model.train_op],feed_dict=feed_dict)\n",
    "        if i==args.num_batches-1:\n",
    "            embedding=sess.run(model.embedding)\n",
    "            normalized_embedding = embedding / np.linalg.norm(embedding, axis=1, keepdims=True)\n",
    "            return data_loader.embedding_mapping(normalized_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Patient-medicine embedding dict\n",
    "embedding_dict = train(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = embedding_dict.keys()\n",
    "patient_embedding = {}\n",
    "medicine_embedding = {}\n",
    "for i in list(keys):\n",
    "    if str(i).isnumeric():\n",
    "        patient_embedding[str(i)] = embedding_dict[i]\n",
    "    else:\n",
    "        medicine_embedding[i] = embedding_dict[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TransD embedding for the drugs of medicine knowledge graph\n",
    "medicine_data = pd.read_csv (\"./pykg2vec/examples/data/embeddings/transd/ent_embedding.tsv\", sep = '\\t', header=None)\n",
    "label_data = pd.read_csv (\"./pykg2vec/examples/data/embeddings/transd/ent_labels.tsv\", sep = '\\t', header=None)\n",
    "medicine_data = medicine_data.to_numpy()\n",
    "label_data = label_data.to_numpy()\n",
    "medicine_kg_embedding = {}\n",
    "for index, label in enumerate(label_data):\n",
    "    medicine_kg_embedding[label[0]] = medicine_data[index]\n",
    "    \n",
    "# TransD embedding for the relations in medicine knowledge graph    \n",
    "med_rel_data = pd.read_csv (\"./pykg2vec/examples/data/embeddings/transd/rel_embedding.tsv\", sep = '\\t', header=None)\n",
    "med_label_data = pd.read_csv (\"./pykg2vec/examples/data/embeddings/transd/rel_labels.tsv\", sep = '\\t', header=None, encoding='unicode_escape')\n",
    "med_rel_data = med_rel_data.to_numpy()\n",
    "med_label_data = med_label_data.to_numpy()\n",
    "med_rel_embedding = {}\n",
    "for index, label in enumerate(med_label_data):\n",
    "    med_rel_embedding[label[0]] = med_rel_data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To construct disease knowledge graph, use the ICD9 Ontology dataset\n",
    "disease_df = pd.read_csv('ICD9CM.csv')\n",
    "disease_df = disease_df[['Class ID','Parents']]\n",
    "disease_df['type'] = 'childOf'\n",
    "disease_df = disease_df.rename(columns={\"Class ID\": \"target\", \"Parents\": \"source\"})\n",
    "disease_df = disease_df[['target','type','source']]\n",
    "disease_df['source'] = disease_df.source.apply(lambda x: str(x).split('/')[-1].replace('.', ''))\n",
    "disease_df['target'] = disease_df.target.apply(lambda x: str(x).split('/')[-1].replace('.', ''))\n",
    "# Split into train, test and valid to calculate the embedding\n",
    "X_train, X_test_val = train_test_split(disease_df, test_size=0.6)\n",
    "X_test, X_val = train_test_split(X_test_val, test_size=0.5)\n",
    "X_train.to_csv('./disease-train.csv',sep='\\t')\n",
    "X_val.to_csv('./disease-valid.csv',sep='\\t')\n",
    "X_test.to_csv('./disease-test.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct patient disease bipartite network\n",
    "disease_patient_df = pd.read_csv('DIAGNOSES_ICD.csv')\n",
    "disease_patient_df = disease_patient_df[['subject_id','icd9_code']]\n",
    "G1 = nx.Graph()\n",
    "G1.add_nodes_from(disease_patient_df['subject_id'], bipartite=0)\n",
    "G1.add_nodes_from(disease_patient_df['icd9_code'], bipartite=1)\n",
    "G1.add_weighted_edges_from(\n",
    "    [(row['subject_id'], row['icd9_code'], 1) for idx, row in disease_patient_df.iterrows()], weight='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_embedding_dict = train(G1)\n",
    "keys = disease_embedding_dict.keys()\n",
    "disease_embedding = {}\n",
    "\n",
    "for i in list(keys):\n",
    "    if str(i).isnumeric():\n",
    "        lists = []\n",
    "        if str(i) in patient_embedding:\n",
    "            lists.append(patient_embedding[str(i)])\n",
    "        lists.append(disease_embedding_dict[i])\n",
    "        # Combine the patient embedding from p-m bipartite network and p-d bipartite network\n",
    "        patient_embedding[str(i)] = list(map(sum, zip(*lists)))\n",
    "    else:\n",
    "        disease_embedding[i] = disease_embedding_dict[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the medicine embedding from p-m bipartite network and medicine knowledge graph\n",
    "for i in medicine_kg_embedding:\n",
    "    if i in medicine_embedding:\n",
    "        lists = []\n",
    "        lists.append(medicine_embedding[i])\n",
    "        lists.append(medicine_kg_embedding[i])\n",
    "        medicine_kg_embedding[i] = list(map(sum, zip(*lists)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the disease embedding from disease knowledge graph and combine with embedding from p-d knowledge graph\n",
    "disease_data = pd.read_csv (\"./pykg2vec/examples/disease-kg/embeddings/transd/ent_embedding.tsv\", sep = '\\t', header=None)\n",
    "disease_label_data = pd.read_csv (\"./pykg2vec/examples/disease-kg/embeddings/transd/ent_labels.tsv\", sep = '\\t', header=None)\n",
    "disease_data = disease_data.to_numpy()\n",
    "disease_label_data = disease_label_data.to_numpy()\n",
    "medicine_kg_embedding = {}\n",
    "for index, label in enumerate(disease_label_data):\n",
    "    lists = []\n",
    "    if label[0] in disease_embedding:\n",
    "        lists.append(disease_embedding[label[0]])\n",
    "    lists.append(disease_data[index])\n",
    "    disease_embedding[label[0]] = list(map(sum, zip(*lists)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 10019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "sepsis_drug = ['Vancomycin','Ceftriaxone','Cefuroxime','Ceftin','Vancocin']\n",
    "hypertension_drug = ['Lisinopril','Amlodipine','Norvasc','Carvedilol','Furosemide']\n",
    "resp_failure_drug = ['Acetaminophen','Norepinephrine','Prednisone','Albuterol','Noxivent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_em = patient_embedding[str(p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(arr1, arr2):\n",
    "    prod = 0\n",
    "    for i, v in enumerate(arr1):\n",
    "        prod = prod + (arr1[i] * arr2[i])\n",
    "    return prod\n",
    "\n",
    "def l1_norm(arr):\n",
    "    norm = 0\n",
    "    for i in arr:\n",
    "        norm = norm + abs(i)\n",
    "    return norm\n",
    "\n",
    "def sum_arr(arr1, arr2):\n",
    "    return_arr = []\n",
    "    for i, v in enumerate(arr1):\n",
    "        return_arr.append(arr1[i] + arr2[i])\n",
    "    return return_arr\n",
    "\n",
    "def diff_arr(arr1, arr2):\n",
    "    return_arr = []\n",
    "    for i, v in enumerate(arr1):\n",
    "        return_arr.append(arr1[i] - arr2[i])\n",
    "    return return_arr\n",
    "\n",
    "def scalar_product(scalar, arr):\n",
    "    return_arr = []\n",
    "    for i in arr:\n",
    "        return_arr.append(i * scalar)\n",
    "    return return_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ddi(patient_em, arr1, arr2):\n",
    "    n = 5\n",
    "    res_scores = {}\n",
    "    \n",
    "    for index, value in enumerate(arr1):\n",
    "        if value in list(medicine_kg_embedding.keys()): \n",
    "            res = dot_product(patient_em, medicine_kg_embedding[value])\n",
    "            scores[value] = res\n",
    "            \n",
    "    for i in range(0, n):\n",
    "        value = arr1[i]\n",
    "        if value in scores:\n",
    "            for j in arr2:\n",
    "                ddi = 0\n",
    "                relation_df = medicine_kg_df[(medicine_kg_df['source'] == value) & (medicine_kg_df['target'] == j)]\n",
    "                relation = relation_df['edge'].unique()\n",
    "                if len(relation) > 0:\n",
    "                    relation = relation[0]\n",
    "                    ddi = ddi + l1_norm(diff_arr(sum_arr(medicine_kg_embedding[value], med_rel_embedding[relation]), medicine_kg_embedding[j]))\n",
    "            res_scores[value] = scores[value] - ddi\n",
    "    return res_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Vancomycin': -554.9162230729705,\n",
       " 'Ceftriaxone': -557.8339226240262,\n",
       " 'Cefuroxime': -558.7119698473205}"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sepsis_ddi = calculate_ddi(patient_em, sepsis_drug, hypertension_drug)\n",
    "sepsis_ddi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lisinopril': -14.056283833129838,\n",
       " 'Amlodipine': 3.3913091023758932,\n",
       " 'Carvedilol': -4.981197779652382,\n",
       " 'Furosemide': 12.222794158188186}"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypertension_ddi = calculate_ddi(patient_em, hypertension_drug, sepsis_drug)\n",
    "hypertension_ddi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Acetaminophen': 34.319806024994,\n",
       " 'Norepinephrine': -4.349440260140723,\n",
       " 'Prednisone': -6.313434044598619}"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp_failure_ddi = calculate_ddi(patient_em, resp_failure_drug, sepsis_drug)\n",
    "resp_failure_ddi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22713131428177216, -0.06436416200443511, -0.06797525378304467, 0.11078386062666923, 0.08790227845927051, 0.030545909622186413, 0.1489080885635613, -0.09045192235260764, -0.12998357313164038, -0.06548483085180652, -0.14461861927186187, 0.04129065622747197, -0.0033810655376894025, -0.06675221620062702, -0.08474981671774566, -0.017934030150872325, -0.05666303840875047, 0.12161384445041852, 0.059725237774324244, 0.30432979887447953, 0.09906956352084226, -0.23363533089997313, -0.002737359841358377, -0.10742571750330182, -0.12811973155023276, 0.13554820451096083, -0.007797355628315655, -0.08919129111385406, 0.09034248716057855, -0.04636084356218427, 0.10136227174155837, 0.018091533589574334, 0.047255239550534844, -0.06489889817160005, 0.027954206776750957, -0.013842769886809839, 0.32765129684885635, -0.055574395194754406, -0.2457427589361605, -0.05581639173345987, -0.07683065207616775, -0.19231008188434578, 0.355714113452659, 0.3364620643825719, 0.11005410031536546, 0.06878400394254308, 0.01804434809041995, 0.002399543654860902, 0.006558955704588594, -0.05302520151134536]\n"
     ]
    }
   ],
   "source": [
    "# 99592 - sepsis\n",
    "# 4019 - unspecified essential hypertension\n",
    "\n",
    "diseases = ['99592', '4019']\n",
    "for i, val in enumerate(diseases):\n",
    "    d = disease_embedding[val]\n",
    "    res_scalar = scalar_product(math.exp(-(i+1)), d)\n",
    "    if i == 0:\n",
    "        new_patient_embedding = res_scalar\n",
    "    else:\n",
    "        new_patient_embedding = sum_arr(new_patient_embedding, res_scalar)\n",
    "print(new_patient_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Vancomycin': -548.910495202641,\n",
       " 'Ceftriaxone': -545.8999233213062,\n",
       " 'Cefuroxime': -561.4798174361738}"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sepsis_ddi = calculate_ddi(new_patient_embedding, sepsis_drug, hypertension_drug)\n",
    "sepsis_ddi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
